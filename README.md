# 최신 자연어처리 모델 소개

2018년 BERT의 등장 이후로 사전훈련 모델들이 딥러닝 자연어처리의 표준으로 자리잡았습니다. 2020년에는 GPT-3가 공개되면서 초거대모델로 다시 흐름이 바뀌고 있습니다. 지금까지 수많은 자연어처리 모델들이 공개되었지만, 한눈에 볼 수 있는 자료가 많지 않았습니다. 그래서 최신 자연어처리 모델들을 정리한 문서를 작성해보았습니다.

각 모델마다 간략한 특징만 나와있는데, 보다 자세한 내용은 <최신 자연어처리 모델 소개.pdf> 문서를 확인하시면 됩니다. Github에서 바로 pdf를 열면 링크가 작동하지 않는 문제가 있습니다. 되도록 다운로드 후 읽어보시길 추천드립니다. 혹시 잘못된 정보가 있거나 추가하고 싶은 모델이 있으면 이슈에 올려주시면 고맙겠습니다. 앞으로 계속 새로운 모델을 업데이트할 예정입니다.


## 모델 목록
- 2018/11
  - **BERT** (구글)
    - 특징
      - Transformer의 Encoder로 만든 사전훈련 모델
    - 모델크기
      - Base 110M / Large 340M
    - 관련문서
      - [The Illustrated BERT, ELMo, and co.](https://nlpinkorean.github.io/illustrated-bert)
      - [The Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)
- 2019/02
  - **GPT-2** (OpenAI)
    - 특징
      - Transformer의 Decoder로 만든 사전훈련 모델
    - 모델크기
      - Small 124M / Medium 355M / Large 774M / XL 1.5B
    - 관련문서
      - [The Illustrated GPT-2](https://chloamme.github.io/2021/12/08/illustrated-gpt2-korean.html)
- 2019/07
  - **RoBERTa** (메타)
    - 특징 
      - BERT를 개선한 모델
    - 모델크기
      - Base 125M / Large 355M
    - 관련문서
      - [RoBERTa Review](https://baekyeongmin.github.io/paper-review/roberta-review/)
- 2019/10
  - **BART** (메타)
    - 특징
      - Seq2Seq 구조로 손상된 텍스트를 복구하는 사전훈련 방법 사용
    - 모델크기
      - Base 140M / Large 400M
    - 관련문서
      - [BART 논문 리뷰](https://dladustn95.github.io/nlp/BART_paper_review/)
  - **KoBERT** (SKT)
    - 특징
      - 한국어 BERT
    - 모델크기
      - 92M
    - 관련문서
      - https://github.com/SKTBrain/KoBERT
- 2019/12
  - **ALBERT** (구글)
    - 특징
      - BERT 경량화
    - 모델크기
      - Base 12M / Large 18M / XLarge 60M / XXLarge 235M
    - 관련문서
      - [ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations](https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html)
      - [ALBERT 논문 리뷰](https://thejb.ai/albert/)
- 2020/01
  - **Meena** (구글)
    - 특징
      - 일상대화 모델
    - 모델크기
      - 2.6B
    - 관련문서
      - [구글의 일상대화 딥러닝 모델 - Meena](http://aidev.co.kr/chatbotdeeplearning/8881)
- 2020/03
  - **ELECTRA** (구글)
    - 특징
      - RTD(Replaced Token Detection) 방식으로 사전훈련
    - 모델크기
      - Small 14M / Base 110M / Large 335M
    - 관련문서
      - [More Efficient NLP Model Pre-training with ELECTRA](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html)
      - [꼼꼼하고 이해하기 쉬운 ELECTRA 논문 리뷰](https://blog.pingpong.us/electra-review/)
- 2020/04
  - **KoELECTRA** (박장원)
    - 특징
      - 한국어 ELECTRA
    - 모델크기
      - Small 14M / Base 110M
    - 관련문서
      - [2주 간의 KoELECTRA 개발기](https://monologg.kr/2020/05/02/koelectra-part1/)
      - https://github.com/monologg/KoELECTRA
  - **KoGPT-2** (SKT)
    - 특징
      - 한국어 GPT-2
    - 모델크기
      - 125M
    - 관련문서
      - https://github.com/SKT-AI/KoGPT2
- 2020/05
  - **GPT-3** (OpenAI)
    - 특징
      - GPT-2의 100배 크기를 가진 초거대모델
    - 모델크기
      - 175B
    - 관련문서
      - [How GPT3 Works](https://chloamme.github.io/2021/12/18/how-gpt3-works-visualizations-animations-korean.html)
      - [GPT-3 데모 사이트](https://gpt3demo.com/)
      - [GPT-3 패러다임을 바꿀 미친 성능의 인공지능 등장 및 활용 사례 10가지](https://www.youtube.com/watch?v=I7sZVrwM6_Q)
  - **BlenderBot** (메타)
    - 특징
      - 일상대화 모델
    - 모델크기
      - 9.4B
    - 관련문서
      - [페이스북의 일상대화 딥러닝 모델 - Blender](http://aidev.co.kr/chatbotdeeplearning/9114)
- 2020/07
  - **KcBERT** (이준범)
    - 특징
      - 구어체에 특화된 한국어 BERT
    - 모델크기
      - Base 108M / Large 334M
    - 관련문서
      - https://github.com/Beomi/KcBERT
- 2020/12
  - **KoBART** (SKT)
    - 특징
      - 한국어 BART
    - 모델크기
      - 124M
    - 관련문서
      - https://github.com/SKT-AI/KoBART
- 2021/01
  - **DALL·E** (OpenAI)
    - 특징
      - Text-to-Image 모델
    - 모델크기
      - 12B
    - 관련문서
      - [DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)
  - **CLIP** (OpenAI)
    - 특징
      - 이미지와 텍스트 임베딩이 유사하도록 만드는 모델
    - 모델크기
      - 미확인
    - 관련문서
      - [OpenAI의 이미지인식 모델 CLIP](http://aidev.co.kr/deeplearning/10254)
- 2021/04
  - **KoELECTRA** (이준범)
    - 특징
      - 구어체에 특화된 한국어 ELECTRA
    - 모델크기
      - 124M
    - 관련문서
      - https://github.com/Beomi/KcELECTRA
- 2021/05
  - **LaMDA** (구글)
    - 특징
      - 대화 전용 초거대모델
    - 모델크기
      - 137B
    - 관련문서
      - [LaMDA: our breakthrough conversation technology](https://blog.google/technology/ai/lamda/)
      - [구글의 대화기반 초거대모델 LaMDA, 논문 공개](http://aidev.co.kr/chatbotdeeplearning/11129)
  - **하이퍼클로바** (네이버)
    - 특징
      - 한국어 초거대모델
    - 모델크기
      - 204B
    - 관련문서
      - [하이퍼클로바 활용예 및 사용가이드](http://aidev.co.kr/chatbotdeeplearning/11213)
- 2021/07
  - **BlenderBot 2.0** (메타)
    - 특징
      - 검색 및 기억 능력이 추가된 일상대화 모델
    - 모델크기
      - 미확인
    - 관련문서
      - [인터넷 검색을 하고 장기기억을 저장하는 페이스북의 챗봇 - Blenderbot2.0](http://aidev.co.kr/chatbotdeeplearning/10629)
- 2021/09
  - **TUNiB-Electra** (튜닙)
    - 특징
      - 한국어 ELECTRA
    - 모델크기
      - Small 14M / Base 110M
    - 관련문서
      - https://github.com/tunib-ai/tunib-electra
- 2021/10
  - **KLUE-BERT** (KLUE)
    - 특징
      - 한국어 BERT
    - 모델크기
      - 110M
    - 관련문서
      - https://github.com/KLUE-benchmark/KLUE
  - **KLUE-RoBERTa** (KLUE)
    - 특징
      - 한국어 RoBERTa
    - 모델크기
      - Base 125M / Large 355M
    - 관련문서
      - https://github.com/KLUE-benchmark/KLUE
- 2021/11
  - **KoGPT** (카카오)
    - 특징
      - 한국어 GPT
    - 모델크기
      - 6B
    - 관련문서
      - https://github.com/kakaobrain/kogpt
- 2021/12
  - **minDALL-E** (카카오)
    - 특징
      - Text-to-Image 모델
    - 모델크기
      - 1.3B
    - 관련문서
      - https://github.com/kakaobrain/minDALL-E
  - **엑사원** (LG)
    - 특징
      - 텍스트와 이미지를 동시에 처리하는 한국어 초거대모델
    - 모델크기
      - 300B
    - 관련문서
      - LG 초거대 AI '엑사원' 등장...언어와 이미지, 한국어와 영어 같이 다뤄(http://www.aitimes.com/news/articleView.html?idxno=141958)
  - **Gopher** (딥마인드)
    - 특징
      - GPT-3보다 큰 초거대모델
    - 모델크기
      - 280B
    - 관련문서
      - [Language modelling at scale: Gopher, ethical considerations, and retrieval](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval)
  - **RETRO** (딥마인드)
    - 7.5B이지만 외부 검색으로 성능을 높인 모델
- 2022/01
  - **InstructGPT** (OpenAI)
    - GPT-3의 업그레이드 버전
- 2022/02
  - **AlphaCode** (딥마인드)
    - 설명이 주어지면 코드를 작성하는 모델
- 2022/04
  - **DALL·E 2** (OpenAI)
    - CLIP과 Diffusion을 사용한 Text-to-Image 모델
  - **Chinchilla** (딥마인드)
    - 70B으로 280B의 Gopher보다 뛰어난 성능을 보임
  - **Flamingo** (딥마인드)
    - 텍스트, 이미지, 영상을 처리할 수 있는 멀티모달 모델
  - **PaLM** (구글)
    - GPT-3의 3배인 540B의 초거대모델
- 2022/05
  - **Imagen** (구글)
    - Diffusion을 사용한 Text-to-Image 모델
  - **Gato** (딥마인드)
    - 텍스트, 이미지, 영상, 게임, 로봇 등 다양한 작업을 하나의 모델로 수행
  - **Parti** (구글)
    - Encoder-Decoder 구조의 Text-to-Image 모델
  - **에이닷** (SKT)
    - 일상대화가 가능한 개인비서 앱
  - **CogVideo** (칭화대)
    - 4초 32프레임의 영상을 만드는 Text-to-Video 모델


## 참고 자료
- 한국어 사전학습 모델
  - https://github.com/sooftware/Korean-PLM
